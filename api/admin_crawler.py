"""
í¬ë¡¤ëŸ¬ ê´€ë¦¬ Admin
- í¬ë¡¤ë§ ì‹¤í–‰
- ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
- ì´ë©”ì¼ ìº í˜ì¸ ë°œì†¡
"""

from django.contrib import admin
from django.contrib.admin.views.decorators import staff_member_required
from django.shortcuts import render, redirect
from django.urls import path, reverse
from django.http import HttpResponse, JsonResponse
from django.utils import timezone
from django.contrib import messages
from django.core.files.base import ContentFile
from django.views.decorators.csrf import csrf_protect
from django.utils.decorators import method_decorator

from .models_crawler import CrawlSession, CrawlResult, EmailCampaign
from .services.crawler_service import (
    run_crawler, run_all_crawlers, export_to_excel,
    get_emails_from_data, CRAWLER_MAP,
    crawl_local_business_emails, get_local_business_categories, get_local_business_regions
)

import json
import logging
from datetime import datetime
from io import BytesIO

logger = logging.getLogger(__name__)


# ============== Admin ModelAdmin ==============

@admin.register(CrawlSession)
class CrawlSessionAdmin(admin.ModelAdmin):
    list_display = ['id', 'crawler_type', 'status', 'total_count', 'email_count', 'created_by', 'created_at', 'completed_at']
    list_filter = ['status', 'crawler_type', 'created_at']
    search_fields = ['id']
    readonly_fields = ['created_at', 'completed_at', 'total_count', 'email_count']
    ordering = ['-created_at']

    actions = ['download_excel']

    def download_excel(self, request, queryset):
        """ì„ íƒí•œ ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ"""
        all_data = []
        for session in queryset:
            for result in session.results.all():
                all_data.append({
                    'ì—…ì¢…': result.get_category_display(),
                    'ì„±ëª…': result.name,
                    'ì‚¬ë¬´ì†Œëª…': result.office_name,
                    'ì†Œì†': result.affiliation,
                    'ì£¼ì†Œ': result.address,
                    'ì§€ì—­': result.region,
                    'ì „í™”ë²ˆí˜¸': result.phone,
                    'ì´ë©”ì¼': result.email,
                    'ì „ë¬¸ë¶„ì•¼': result.specialty,
                })

        if not all_data:
            self.message_user(request, "ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", messages.WARNING)
            return

        excel_file = export_to_excel(all_data)
        if excel_file:
            response = HttpResponse(
                excel_file.read(),
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename="crawl_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx"'
            return response

    download_excel.short_description = "ì„ íƒí•œ ì„¸ì…˜ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ"


@admin.register(CrawlResult)
class CrawlResultAdmin(admin.ModelAdmin):
    list_display = ['id', 'category', 'name', 'office_name', 'region', 'phone', 'email', 'email_sent', 'created_at']
    list_filter = ['category', 'region', 'email_sent', 'created_at']
    search_fields = ['name', 'office_name', 'email', 'phone']
    readonly_fields = ['created_at']
    ordering = ['-created_at']

    actions = ['export_selected', 'mark_email_sent']

    def export_selected(self, request, queryset):
        """ì„ íƒí•œ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ"""
        data = []
        for result in queryset:
            data.append({
                'ì—…ì¢…': result.get_category_display(),
                'ì„±ëª…': result.name,
                'ì‚¬ë¬´ì†Œëª…': result.office_name,
                'ì†Œì†': result.affiliation,
                'ì£¼ì†Œ': result.address,
                'ì§€ì—­': result.region,
                'ì „í™”ë²ˆí˜¸': result.phone,
                'ì´ë©”ì¼': result.email,
                'ì „ë¬¸ë¶„ì•¼': result.specialty,
            })

        excel_file = export_to_excel(data)
        if excel_file:
            response = HttpResponse(
                excel_file.read(),
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename="selected_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx"'
            return response

    export_selected.short_description = "ì„ íƒí•œ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ"

    def mark_email_sent(self, request, queryset):
        """ì„ íƒí•œ ê²°ê³¼ë¥¼ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œë¡œ í‘œì‹œ"""
        count = queryset.update(email_sent=True, email_sent_at=timezone.now())
        self.message_user(request, f"{count}ê±´ì´ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œë¡œ í‘œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.")

    mark_email_sent.short_description = "ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œë¡œ í‘œì‹œ"


@admin.register(EmailCampaign)
class EmailCampaignAdmin(admin.ModelAdmin):
    list_display = ['id', 'name', 'subject', 'status', 'total_count', 'success_count', 'fail_count', 'created_at']
    list_filter = ['status', 'created_at']
    search_fields = ['name', 'subject']
    readonly_fields = ['total_count', 'success_count', 'fail_count', 'started_at', 'completed_at', 'created_at']
    ordering = ['-created_at']


# ============== í¬ë¡¤ëŸ¬ ê´€ë¦¬ ë·° ==============

@staff_member_required
def crawler_dashboard(request):
    """í¬ë¡¤ëŸ¬ ëŒ€ì‹œë³´ë“œ"""
    recent_sessions = CrawlSession.objects.all()[:10]
    total_results = CrawlResult.objects.count()
    total_emails = CrawlResult.objects.exclude(email='').count()

    # ì—…ì¢…ë³„ í†µê³„
    category_stats = {}
    for key, (name, _) in CRAWLER_MAP.items():
        count = CrawlResult.objects.filter(category=key).count()
        email_count = CrawlResult.objects.filter(category=key).exclude(email='').count()
        category_stats[name] = {'count': count, 'email_count': email_count}

    context = {
        'title': 'í¬ë¡¤ëŸ¬ ê´€ë¦¬',
        'recent_sessions': recent_sessions,
        'total_results': total_results,
        'total_emails': total_emails,
        'category_stats': category_stats,
        'crawler_types': CRAWLER_MAP,
    }
    return render(request, 'admin/crawler/dashboard.html', context)


@staff_member_required
@csrf_protect
def run_crawler_view(request):
    """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    if request.method == 'POST':
        crawler_type = request.POST.get('crawler_type', 'all')
        regions = request.POST.getlist('regions')
        max_pages = int(request.POST.get('max_pages', 5))

        # LocalBusiness í¬ë¡¤ëŸ¬ì¸ ê²½ìš°
        if crawler_type == 'local_business':
            category_id = request.POST.get('lb_category_id')
            region_name = request.POST.get('lb_region_name', '').strip()
            limit = int(request.POST.get('lb_limit', 100))

            # ì„¸ì…˜ ìƒì„±
            session = CrawlSession.objects.create(
                crawler_type='local_business',
                regions=[region_name] if region_name else [],
                max_pages=limit,
                status='running',
                created_by=request.user
            )

            try:
                # LocalBusiness í¬ë¡¤ë§ ì‹¤í–‰
                result = crawl_local_business_emails(
                    category_id=int(category_id) if category_id else None,
                    region_name=region_name if region_name else None,
                    limit=limit
                )

                all_data = result['data']
                session.total_count = result.get('businesses_crawled', 0)
                session.email_count = result['email_count']

                # ê²°ê³¼ ì €ì¥
                for item in all_data:
                    CrawlResult.objects.create(
                        session=session,
                        category='local_business',
                        name=item.get('ì—…ì²´ëª…', ''),
                        office_name=item.get('ì—…ì²´ëª…', ''),
                        affiliation='',
                        address=item.get('ì£¼ì†Œ', ''),
                        region=item.get('ì§€ì—­', ''),
                        phone=item.get('ì „í™”ë²ˆí˜¸', ''),
                        email=item.get('ì´ë©”ì¼', ''),
                        specialty=item.get('ì›¹ì‚¬ì´íŠ¸', ''),
                    )

                # ì—‘ì…€ íŒŒì¼ ìƒì„± ë° ì €ì¥
                excel_file = export_to_excel(all_data)
                if excel_file:
                    filename = f"crawl_local_business_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                    session.result_file.save(filename, ContentFile(excel_file.read()))

                session.status = 'completed'
                session.completed_at = timezone.now()
                session.save()

                messages.success(
                    request,
                    f"í¬ë¡¤ë§ ì™„ë£Œ! {result.get('businesses_crawled', 0)}ê°œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ {session.email_count}ê°œ ì´ë©”ì¼ ìˆ˜ì§‘"
                )

            except Exception as e:
                logger.error(f"LocalBusiness í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                session.status = 'failed'
                session.error_message = str(e)
                session.save()
                messages.error(request, f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            return redirect('admin_crawler_dashboard')

        # ê¸°ì¡´ í˜‘íšŒ í¬ë¡¤ëŸ¬
        if not regions:
            regions = ['ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ']

        # ì„¸ì…˜ ìƒì„±
        session = CrawlSession.objects.create(
            crawler_type=crawler_type,
            regions=regions,
            max_pages=max_pages,
            status='running',
            created_by=request.user
        )

        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            if crawler_type == 'all':
                result = run_all_crawlers(regions=regions, max_pages=max_pages)
                all_data = result['all_data']
                session.total_count = result['total_count']
                session.email_count = result['total_emails']
            else:
                result = run_crawler(crawler_type, regions=regions, max_pages=max_pages)
                all_data = result['data']
                session.total_count = result['count']
                session.email_count = result['email_count']

            # ê²°ê³¼ ì €ì¥
            for item in all_data:
                # ì—…ì¢… ë§¤í•‘
                category = None
                category_name = item.get('ì—…ì¢…', '')
                for key, (name, _) in CRAWLER_MAP.items():
                    if name == category_name:
                        category = key
                        break

                if category:
                    CrawlResult.objects.create(
                        session=session,
                        category=category,
                        name=item.get('ì„±ëª…', '') or item.get('ëŒ€í‘œì', ''),
                        office_name=item.get('ì‚¬ë¬´ì†Œëª…', ''),
                        affiliation=item.get('ì†Œì†', ''),
                        address=item.get('ì£¼ì†Œ', ''),
                        region=item.get('ì§€ì—­', ''),
                        phone=item.get('ì „í™”ë²ˆí˜¸', ''),
                        email=item.get('ì´ë©”ì¼', ''),
                        specialty=item.get('ì „ë¬¸ë¶„ì•¼', ''),
                    )

            # ì—‘ì…€ íŒŒì¼ ìƒì„± ë° ì €ì¥
            excel_file = export_to_excel(all_data)
            if excel_file:
                filename = f"crawl_{crawler_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                session.result_file.save(filename, ContentFile(excel_file.read()))

            session.status = 'completed'
            session.completed_at = timezone.now()
            session.save()

            messages.success(request, f"í¬ë¡¤ë§ ì™„ë£Œ! {session.total_count}ê±´ ìˆ˜ì§‘ (ì´ë©”ì¼ {session.email_count}ê±´)")

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            session.status = 'failed'
            session.error_message = str(e)
            session.save()
            messages.error(request, f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

        return redirect('admin_crawler_dashboard')

    # GET ìš”ì²­
    # LocalBusiness ì¹´í…Œê³ ë¦¬ ë° ì§€ì—­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    lb_categories = get_local_business_categories()
    lb_regions = get_local_business_regions()

    context = {
        'title': 'í¬ë¡¤ëŸ¬ ì‹¤í–‰',
        'crawler_types': [('all', 'ì „ì²´ (í˜‘íšŒ)')] + [(k, v[0]) for k, v in CRAWLER_MAP.items()] + [('local_business', 'ğŸ“ DB ì—…ì²´ ì›¹ì‚¬ì´íŠ¸')],
        'regions': ['ì„œìš¸', 'ê²½ê¸°', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
                    'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'],
        'lb_categories': lb_categories,
        'lb_regions': lb_regions,
    }
    return render(request, 'admin/crawler/run_crawler.html', context)


@staff_member_required
def download_session_excel(request, session_id):
    """ì„¸ì…˜ ê²°ê³¼ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ"""
    try:
        session = CrawlSession.objects.get(pk=session_id)

        # ì €ì¥ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ë°˜í™˜
        if session.result_file:
            response = HttpResponse(
                session.result_file.read(),
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename="{session.result_file.name}"'
            return response

        # ì—†ìœ¼ë©´ ìƒì„±
        data = []
        for result in session.results.all():
            data.append({
                'ì—…ì¢…': result.get_category_display(),
                'ì„±ëª…': result.name,
                'ì‚¬ë¬´ì†Œëª…': result.office_name,
                'ì†Œì†': result.affiliation,
                'ì£¼ì†Œ': result.address,
                'ì§€ì—­': result.region,
                'ì „í™”ë²ˆí˜¸': result.phone,
                'ì´ë©”ì¼': result.email,
                'ì „ë¬¸ë¶„ì•¼': result.specialty,
            })

        excel_file = export_to_excel(data)
        if excel_file:
            response = HttpResponse(
                excel_file.read(),
                content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
            )
            response['Content-Disposition'] = f'attachment; filename="session_{session_id}_{datetime.now().strftime("%Y%m%d")}.xlsx"'
            return response

        messages.warning(request, "ë‹¤ìš´ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return redirect('admin_crawler_dashboard')

    except CrawlSession.DoesNotExist:
        messages.error(request, "ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return redirect('admin_crawler_dashboard')


@staff_member_required
def email_campaign_create(request):
    """ì´ë©”ì¼ ìº í˜ì¸ ìƒì„±"""
    if request.method == 'POST':
        name = request.POST.get('name')
        subject = request.POST.get('subject')
        content = request.POST.get('content')
        categories = request.POST.getlist('categories')
        regions = request.POST.getlist('regions')

        # ìº í˜ì¸ ìƒì„±
        campaign = EmailCampaign.objects.create(
            name=name,
            subject=subject,
            content=content,
            target_categories=categories,
            target_regions=regions,
            created_by=request.user,
        )

        # ëŒ€ìƒ ì„¤ì •
        queryset = CrawlResult.objects.exclude(email='')
        if categories:
            queryset = queryset.filter(category__in=categories)
        if regions:
            queryset = queryset.filter(region__in=regions)

        campaign.target_results.set(queryset)
        campaign.total_count = queryset.count()
        campaign.save()

        messages.success(request, f"ìº í˜ì¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€ìƒ: {campaign.total_count}ê±´")
        return redirect('admin:api_emailcampaign_change', campaign.pk)

    # GET
    categories = [(k, v[0]) for k, v in CRAWLER_MAP.items()]
    regions = CrawlResult.objects.values_list('region', flat=True).distinct()

    context = {
        'title': 'ì´ë©”ì¼ ìº í˜ì¸ ìƒì„±',
        'categories': categories,
        'regions': list(regions),
    }
    return render(request, 'admin/crawler/email_campaign.html', context)


@staff_member_required
def send_email_campaign(request, campaign_id):
    """ì´ë©”ì¼ ìº í˜ì¸ ë°œì†¡"""
    try:
        from .utils.email_sender import EmailSender

        campaign = EmailCampaign.objects.get(pk=campaign_id)

        if campaign.status not in ['draft', 'failed']:
            messages.warning(request, "ì´ë¯¸ ë°œì†¡ ì¤‘ì´ê±°ë‚˜ ì™„ë£Œëœ ìº í˜ì¸ì…ë‹ˆë‹¤.")
            return redirect('admin:api_emailcampaign_change', campaign.pk)

        campaign.status = 'sending'
        campaign.started_at = timezone.now()
        campaign.save()

        email_sender = EmailSender()
        success = 0
        fail = 0

        for result in campaign.target_results.all():
            if not result.email:
                continue

            try:
                # ì´ë©”ì¼ ë°œì†¡
                email_sender.send_email(
                    to_email=result.email,
                    subject=campaign.subject,
                    body=campaign.content,
                )
                success += 1
                result.email_sent = True
                result.email_sent_at = timezone.now()
                result.save()
            except Exception as e:
                logger.error(f"ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨ ({result.email}): {e}")
                fail += 1

        campaign.success_count = success
        campaign.fail_count = fail
        campaign.status = 'completed'
        campaign.completed_at = timezone.now()
        campaign.save()

        messages.success(request, f"ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ! ì„±ê³µ: {success}ê±´, ì‹¤íŒ¨: {fail}ê±´")

    except EmailCampaign.DoesNotExist:
        messages.error(request, "ìº í˜ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ìº í˜ì¸ ë°œì†¡ ì˜¤ë¥˜: {e}")
        messages.error(request, f"ë°œì†¡ ì˜¤ë¥˜: {e}")

    return redirect('admin:api_emailcampaign_changelist')


# ============== URL íŒ¨í„´ ==============
def get_crawler_urls():
    """í¬ë¡¤ëŸ¬ ê´€ë ¨ URL íŒ¨í„´ ë°˜í™˜"""
    return [
        path('admin/crawler/', crawler_dashboard, name='admin_crawler_dashboard'),
        path('admin/crawler/run/', run_crawler_view, name='admin_crawler_run'),
        path('admin/crawler/download/<int:session_id>/', download_session_excel, name='admin_crawler_download'),
        path('admin/crawler/email/create/', email_campaign_create, name='admin_crawler_email_create'),
        path('admin/crawler/email/send/<int:campaign_id>/', send_email_campaign, name='admin_crawler_email_send'),
    ]
